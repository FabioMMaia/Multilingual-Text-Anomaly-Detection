{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6KkQ_rdU0bXH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 10924,
     "status": "ok",
     "timestamp": 1747571202579,
     "user": {
      "displayName": "Fabio Maia",
      "userId": "07087344496436365878"
     },
     "user_tz": 180
    },
    "id": "6KkQ_rdU0bXH",
    "outputId": "8a39654e-5fb2-442d-d266-364691b74f02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: deepod in /usr/local/lib/python3.11/dist-packages (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.11/dist-packages (from deepod) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.5.1 in /usr/local/lib/python3.11/dist-packages (from deepod) (1.15.3)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from deepod) (1.6.1)\n",
      "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from deepod) (2.2.2)\n",
      "Requirement already satisfied: torch<1.13.1,>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from deepod) (1.13.0)\n",
      "Requirement already satisfied: tqdm>=4.62.3 in /usr/local/lib/python3.11/dist-packages (from deepod) (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->deepod) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->deepod) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->deepod) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->deepod) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->deepod) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch<1.13.1,>=1.10.0->deepod) (4.13.2)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch<1.13.1,>=1.10.0->deepod) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.11/dist-packages (from torch<1.13.1,>=1.10.0->deepod) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.11/dist-packages (from torch<1.13.1,>=1.10.0->deepod) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch<1.13.1,>=1.10.0->deepod) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<1.13.1,>=1.10.0->deepod) (75.2.0)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<1.13.1,>=1.10.0->deepod) (0.45.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->deepod) (1.17.0)\n",
      "Collecting numpy==1.24.4\n",
      "  Using cached numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Using cached numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.4\n",
      "    Uninstalling numpy-1.24.4:\n",
      "      Successfully uninstalled numpy-1.24.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n",
      "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 1.13.0 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.4 which is incompatible.\n",
      "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.4 which is incompatible.\n",
      "pymc 5.22.0 requires numpy>=1.25.0, but you have numpy 1.24.4 which is incompatible.\n",
      "accelerate 1.6.0 requires torch>=2.0.0, but you have torch 1.13.0 which is incompatible.\n",
      "blosc2 3.3.2 requires numpy>=1.26, but you have numpy 1.24.4 which is incompatible.\n",
      "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n",
      "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.24.4\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "a6ba2a5df20e43359556fd8dd15c7c65",
       "pip_warning": {
        "packages": [
         "numpy"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyod\n",
      "  Using cached pyod-2.0.5-py3-none-any.whl.metadata (46 kB)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from pyod) (1.5.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from pyod) (3.10.0)\n",
      "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.11/dist-packages (from pyod) (1.24.4)\n",
      "Requirement already satisfied: numba>=0.51 in /usr/local/lib/python3.11/dist-packages (from pyod) (0.60.0)\n",
      "Requirement already satisfied: scipy>=1.5.1 in /usr/local/lib/python3.11/dist-packages (from pyod) (1.15.3)\n",
      "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from pyod) (1.6.1)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51->pyod) (0.43.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.0->pyod) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyod) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyod) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyod) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyod) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyod) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyod) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyod) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyod) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->pyod) (1.17.0)\n",
      "Using cached pyod-2.0.5-py3-none-any.whl (200 kB)\n",
      "Installing collected packages: pyod\n",
      "Successfully installed pyod-2.0.5\n"
     ]
    }
   ],
   "source": [
    "# necessary libs to run deepod and pyod\n",
    "!pip install pyod\n",
    "!pip install deepod\n",
    "!pip install numpy==1.24.4 --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e342e405",
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1747571220072,
     "user": {
      "displayName": "Fabio Maia",
      "userId": "07087344496436365878"
     },
     "user_tz": 180
    },
    "id": "e342e405"
   },
   "outputs": [],
   "source": [
    "running='colab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d10QBPcBhe54",
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1747571221698,
     "user": {
      "displayName": "Fabio Maia",
      "userId": "07087344496436365878"
     },
     "user_tz": 180
    },
    "id": "d10QBPcBhe54"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "Em5P0w7NuQG5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1350,
     "status": "ok",
     "timestamp": 1747571245136,
     "user": {
      "displayName": "Fabio Maia",
      "userId": "07087344496436365878"
     },
     "user_tz": 180
    },
    "id": "Em5P0w7NuQG5",
    "outputId": "d5db9955-7354-444c-ce9f-53d67fbb1d29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "if running=='local':\n",
    "    os.chdir(os.path.dirname(os.getcwd()))\n",
    "    project_path = os.getcwd()\n",
    "elif running=='colab':\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    project_path_colab = \"/content/drive/MyDrive/Projeto ML/2025/AD/second_setup\"\n",
    "    repo_name = \"adaptative-text-anomaly-detection\"\n",
    "    project_path = os.path.join(project_path_colab, repo_name)\n",
    "    os.chdir(project_path)\n",
    "else:\n",
    "    raise ValueError(\"Invalid running environment. Choose 'local' or 'colab'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1211dfa1",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1747571245816,
     "user": {
      "displayName": "Fabio Maia",
      "userId": "07087344496436365878"
     },
     "user_tz": 180
    },
    "id": "1211dfa1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "sys.path.append(r'../src')\n",
    "sys.path.append(r'./src')\n",
    "\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5216b22b",
   "metadata": {
    "executionInfo": {
     "elapsed": 800,
     "status": "ok",
     "timestamp": 1747571247757,
     "user": {
      "displayName": "Fabio Maia",
      "userId": "07087344496436365878"
     },
     "user_tz": 180
    },
    "id": "5216b22b"
   },
   "outputs": [],
   "source": [
    "from pipeline.anomaly_detection import label_normal_vs_anomaly,adjust_contamination, pretty_print_data_info, split_data\n",
    "from pipeline.benchmark_runner import benchmark_unsupervised_models, benchmark_semisupervised_models\n",
    "from models.MLP import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86cbc388",
   "metadata": {
    "executionInfo": {
     "elapsed": 2638,
     "status": "ok",
     "timestamp": 1747571252129,
     "user": {
      "displayName": "Fabio Maia",
      "userId": "07087344496436365878"
     },
     "user_tz": 180
    },
    "id": "86cbc388"
   },
   "outputs": [],
   "source": [
    "from deepod.models import DeepSAD, DeepSVDD, DevNet\n",
    "from pyod.models.xgbod import XGBOD\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.xgbod import XGBOD\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "from pyod.models.auto_encoder import AutoEncoder\n",
    "from pyod.models.vae import VAE\n",
    "from pyod.models.pca import PCA\n",
    "from pyod.models.kde import KDE\n",
    "from pyod.models.hbos import HBOS\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NcZLc39Kvwtb",
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1747571439040,
     "user": {
      "displayName": "Fabio Maia",
      "userId": "07087344496436365878"
     },
     "user_tz": 180
    },
    "id": "NcZLc39Kvwtb"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"en\": {\n",
    "        \"datasets\": [\n",
    "            \"tweets-hate-speech-detection/tweets_hate_speech_detection\",\n",
    "            \"SetFit/20_newsgroups\",\n",
    "            \"cardiffnlp/tweet_eval\"\n",
    "        ],\n",
    "        \"encoders\": [\n",
    "            \"sentence-transformers/distiluse-base-multilingual-cased-v1\",\n",
    "            \"sentence-transformers/distiluse-base-multilingual-cased-v2\",\n",
    "            \"FacebookAI/xlm-roberta-large\"\n",
    "        ]\n",
    "    },\n",
    "    \"pt\": {\n",
    "        \"datasets\": [\n",
    "            \"JAugusto97/told-br\",\n",
    "            \"wikinews\",\n",
    "            \"augustop/portuguese-tweets-for-sentiment-analysis\"\n",
    "        ],\n",
    "        \"encoders\": [\n",
    "            \"sentence-transformers/distiluse-base-multilingual-cased-v1\",\n",
    "            \"sentence-transformers/distiluse-base-multilingual-cased-v2\",\n",
    "            \"FacebookAI/xlm-roberta-large\",\n",
    "            \"neuralmind/bert-base-portuguese-cased\",\n",
    "            \"neuralmind/bert-large-portuguese-cased\",\n",
    "            \"PORTULAN/serafim-100m-portuguese-pt-sentence-encoder-ir\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "model_groups = {\n",
    "    \"semi\": {\n",
    "        \"models\": {\n",
    "            \"DevNet\": lambda: DevNet(),\n",
    "            \"DeepSAD\": lambda: DeepSAD(epochs=100, rep_dim=128, device='cuda'),\n",
    "            \"XGBOD\": lambda: XGBOD(estimator_list=[LOF(), IForest()]),\n",
    "            \"MLP\": lambda: MLP()\n",
    "        },\n",
    "        \"benchmark_fn\": benchmark_semisupervised_models,\n",
    "        \"extra_args\": {\n",
    "            \"contamination_levels\": [0.05],\n",
    "            \"n_rounds\": 1\n",
    "        },\n",
    "        \"wrap_model\": lambda name, fn: {name: (fn, False)}  # unpack into (model_fn, uncertainty)\n",
    "    },\n",
    "    \"unsupervised\": {\n",
    "        \"models\": {\n",
    "            \"IForest\": lambda: IForest(),\n",
    "            \"LOF\": lambda: LOF(),\n",
    "            \"DeepSVDD\": lambda: DeepSVDD(epochs=100, rep_dim=128),\n",
    "            \"OCSVM\": lambda: OCSVM(kernel='rbf', nu=0.05, gamma='scale'),\n",
    "            \"OneClassSVM\": lambda: OneClassSVM(kernel='rbf', nu=0.05, gamma='scale'),\n",
    "            \"AutoEncoder\": lambda: AutoEncoder(),\n",
    "            \"VAE\": lambda: VAE(),\n",
    "            \"HBOS\": lambda: HBOS()\n",
    "        },\n",
    "        \"benchmark_fn\": benchmark_unsupervised_models,\n",
    "        \"extra_args\": {},\n",
    "        \"wrap_model\": lambda name, fn: {name: fn}  # no need to wrap with (fn, uncertainty)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05f1a267",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2692029,
     "status": "ok",
     "timestamp": 1747574207152,
     "user": {
      "displayName": "Fabio Maia",
      "userId": "07087344496436365878"
     },
     "user_tz": 180
    },
    "id": "05f1a267",
    "outputId": "44776693-eec1-4d63-f8e2-3e72245ed898"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rBenchmarking dataset/encoder pairs:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: tweets_hate_speech_detection_distiluse-base-multilingual-cased-v1\n",
      "✅ Selected normal label (inlier → 0): 0\n",
      "⚠️  Anomaly labels (outlier → 1): [-1, 1]\n",
      "Shape of embeddings: (31206, 512)\n",
      "Shape of labels: (31206,)\n",
      "Number of anomalies: 1486\n",
      "Number of normal: 29720\n",
      "Percentage of anomalies: 0.047619047619047616\n",
      "Shape of texts: 31206\n",
      "Train set: (24964, 512) (with 1189 outliers and 23775 inliers)\n",
      "Test  set: (6242, 512) (with 297 outliers and 5945 inliers)\n",
      "Running semi model: DevNet on tweets_hate_speech_detection_distiluse-base-multilingual-cased-v1\n",
      "→ Running DevNet (uncertainty_model=False)\n",
      "Start Training...\n",
      "ensemble size: 1\n",
      "MLPnet(\n",
      "  (network): Sequential(\n",
      "    (0): LinearBlock(\n",
      "      (linear): Linear(in_features=512, out_features=100, bias=False)\n",
      "      (act_layer): ReLU()\n",
      "    )\n",
      "    (1): LinearBlock(\n",
      "      (linear): Linear(in_features=100, out_features=50, bias=False)\n",
      "      (act_layer): ReLU()\n",
      "    )\n",
      "    (2): LinearBlock(\n",
      "      (linear): Linear(in_features=50, out_features=1, bias=False)\n",
      "      (act_layer): Identity()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "epoch  1, training loss: 2.483100, time: 3.3s\n",
      "epoch 10, training loss: 2.474825, time: 1.2s\n",
      "epoch 20, training loss: 2.488738, time: 0.9s\n",
      "epoch 30, training loss: 2.513506, time: 0.9s\n",
      "epoch 40, training loss: 2.511513, time: 0.9s\n",
      "epoch 50, training loss: 2.519772, time: 1.1s\n",
      "epoch 60, training loss: 2.476604, time: 0.9s\n",
      "epoch 70, training loss: 2.464977, time: 0.9s\n",
      "epoch 80, training loss: 2.483079, time: 1.1s\n",
      "epoch 90, training loss: 2.498699, time: 0.9s\n",
      "epoch100, training loss: 2.507611, time: 0.9s\n",
      "Start Inference on the training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "testing:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "testing:  40%|████      | 157/391 [00:00<00:00, 1564.09it/s]\u001b[A\n",
      "testing: 100%|██████████| 391/391 [00:00<00:00, 1500.57it/s]\n",
      "\n",
      "testing:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "testing:  32%|███▏      | 125/391 [00:00<00:00, 1245.53it/s]\u001b[A\n",
      "testing: 100%|██████████| 391/391 [00:00<00:00, 1335.84it/s]\n",
      "\n",
      "testing: 100%|██████████| 98/98 [00:00<00:00, 1528.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished DevNet in 104.47 seconds\n",
      "Running semi model: DeepSAD on tweets_hate_speech_detection_distiluse-base-multilingual-cased-v1\n",
      "→ Running DeepSAD (uncertainty_model=False)\n",
      "Start Training...\n",
      "ensemble size: 1\n",
      "training data counter: Counter({0: 23775, -1: 1189})\n",
      "MLPnet(\n",
      "  (network): Sequential(\n",
      "    (0): LinearBlock(\n",
      "      (linear): Linear(in_features=512, out_features=100, bias=False)\n",
      "      (act_layer): ReLU()\n",
      "    )\n",
      "    (1): LinearBlock(\n",
      "      (linear): Linear(in_features=100, out_features=50, bias=False)\n",
      "      (act_layer): ReLU()\n",
      "    )\n",
      "    (2): LinearBlock(\n",
      "      (linear): Linear(in_features=50, out_features=128, bias=False)\n",
      "      (act_layer): Identity()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "epoch  1, training loss: 0.854045, time: 0.9s\n",
      "epoch 10, training loss: 0.307587, time: 0.9s\n",
      "epoch 20, training loss: 0.184367, time: 0.9s\n",
      "epoch 30, training loss: 0.265096, time: 0.9s\n",
      "epoch 40, training loss: 0.160461, time: 0.9s\n",
      "epoch 50, training loss: 0.103714, time: 0.9s\n",
      "epoch 60, training loss: 0.204419, time: 1.1s\n",
      "epoch 70, training loss: 0.130239, time: 0.9s\n",
      "epoch 80, training loss: 0.119991, time: 0.9s\n",
      "epoch 90, training loss: 0.182986, time: 1.1s\n",
      "epoch100, training loss: 0.112373, time: 0.9s\n",
      "Start Inference on the training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "testing:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "testing:  39%|███▉      | 153/391 [00:00<00:00, 1525.32it/s]\u001b[A\n",
      "testing: 100%|██████████| 391/391 [00:00<00:00, 1460.64it/s]\n",
      "\n",
      "testing:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "testing:  31%|███▏      | 123/391 [00:00<00:00, 1225.71it/s]\u001b[A\n",
      "testing: 100%|██████████| 391/391 [00:00<00:00, 1369.03it/s]\n",
      "\n",
      "testing: 100%|██████████| 98/98 [00:00<00:00, 1254.16it/s]\n",
      "/usr/local/lib/python3.11/dist-packages/pyod/models/base.py:554: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished DeepSAD in 95.50 seconds\n",
      "Running semi model: XGBOD on tweets_hate_speech_detection_distiluse-base-multilingual-cased-v1\n",
      "→ Running XGBOD (uncertainty_model=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [12:36:28] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished XGBOD in 54.02 seconds\n",
      "Running semi model: MLP on tweets_hate_speech_detection_distiluse-base-multilingual-cased-v1\n",
      "→ Running MLP (uncertainty_model=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished MLP in 67.52 seconds\n",
      "Running unsupervised model: IForest on tweets_hate_speech_detection_distiluse-base-multilingual-cased-v1\n",
      "→ Running IForest [unsupervised]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished IForest in 1.14 seconds\n",
      "Running unsupervised model: LOF on tweets_hate_speech_detection_distiluse-base-multilingual-cased-v1\n",
      "→ Running LOF [unsupervised]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished LOF in 41.61 seconds\n",
      "Running unsupervised model: DeepSVDD on tweets_hate_speech_detection_distiluse-base-multilingual-cased-v1\n",
      "→ Running DeepSVDD [unsupervised]\n",
      "Start Training...\n",
      "ensemble size: 1\n",
      "MLPnet(\n",
      "  (network): Sequential(\n",
      "    (0): LinearBlock(\n",
      "      (linear): Linear(in_features=512, out_features=100, bias=False)\n",
      "      (act_layer): ReLU()\n",
      "    )\n",
      "    (1): LinearBlock(\n",
      "      (linear): Linear(in_features=100, out_features=50, bias=False)\n",
      "      (act_layer): ReLU()\n",
      "    )\n",
      "    (2): LinearBlock(\n",
      "      (linear): Linear(in_features=50, out_features=128, bias=False)\n",
      "      (act_layer): Identity()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "epoch  1, training loss: 0.046177, time: 0.8s\n",
      "epoch 10, training loss: 0.001147, time: 0.8s\n",
      "epoch 20, training loss: 0.000806, time: 0.8s\n",
      "epoch 30, training loss: 0.000752, time: 1.0s\n",
      "epoch 40, training loss: 0.000652, time: 0.7s\n",
      "epoch 50, training loss: 0.000656, time: 0.7s\n",
      "epoch 60, training loss: 0.000633, time: 0.7s\n",
      "epoch 70, training loss: 0.000531, time: 0.7s\n",
      "epoch 80, training loss: 0.000547, time: 0.9s\n",
      "epoch 90, training loss: 0.000585, time: 0.7s\n",
      "epoch100, training loss: 0.000553, time: 1.0s\n",
      "Start Inference on the training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "testing:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "testing:  31%|███       | 120/391 [00:00<00:00, 1194.00it/s]\u001b[A\n",
      "testing: 100%|██████████| 391/391 [00:00<00:00, 1328.52it/s]\n",
      "\n",
      "testing:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "testing:  38%|███▊      | 150/391 [00:00<00:00, 1496.19it/s]\u001b[A\n",
      "testing: 100%|██████████| 391/391 [00:00<00:00, 1455.63it/s]\n",
      "\n",
      "testing: 100%|██████████| 98/98 [00:00<00:00, 1440.78it/s]\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished DeepSVDD in 77.76 seconds\n",
      "Running unsupervised model: OCSVM on tweets_hate_speech_detection_distiluse-base-multilingual-cased-v1\n",
      "→ Running OCSVM [unsupervised]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished OCSVM in 55.40 seconds\n",
      "Running unsupervised model: OneClassSVM on tweets_hate_speech_detection_distiluse-base-multilingual-cased-v1\n",
      "→ Running OneClassSVM [unsupervised]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished OneClassSVM in 42.35 seconds\n",
      "Running unsupervised model: AutoEncoder on tweets_hate_speech_detection_distiluse-base-multilingual-cased-v1\n",
      "→ Running AutoEncoder [unsupervised]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Training:  10%|█         | 1/10 [00:02<00:26,  2.90s/it]\u001b[A\n",
      "Training:  20%|██        | 2/10 [00:05<00:23,  2.89s/it]\u001b[A\n",
      "Training:  30%|███       | 3/10 [00:08<00:20,  2.89s/it]\u001b[A\n",
      "Training:  40%|████      | 4/10 [00:12<00:19,  3.18s/it]\u001b[A\n",
      "Training:  50%|█████     | 5/10 [00:15<00:15,  3.07s/it]\u001b[A\n",
      "Training:  60%|██████    | 6/10 [00:18<00:12,  3.01s/it]\u001b[A\n",
      "Training:  70%|███████   | 7/10 [00:20<00:08,  2.97s/it]\u001b[A\n",
      "Training:  80%|████████  | 8/10 [00:24<00:06,  3.09s/it]\u001b[A\n",
      "Training:  90%|█████████ | 9/10 [00:27<00:03,  3.11s/it]\u001b[A\n",
      "Training: 100%|██████████| 10/10 [00:30<00:00,  3.03s/it]\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished AutoEncoder in 35.05 seconds\n",
      "Running unsupervised model: VAE on tweets_hate_speech_detection_distiluse-base-multilingual-cased-v1\n",
      "→ Running VAE [unsupervised]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "Training:   3%|▎         | 1/30 [00:05<02:37,  5.43s/it]\u001b[A\n",
      "Training:   7%|▋         | 2/30 [00:10<02:18,  4.95s/it]\u001b[A\n",
      "Training:  10%|█         | 3/30 [00:14<02:11,  4.88s/it]\u001b[A\n",
      "Training:  13%|█▎        | 4/30 [00:19<02:09,  4.96s/it]\u001b[A\n",
      "Training:  17%|█▋        | 5/30 [00:24<01:59,  4.76s/it]\u001b[A\n",
      "Training:  20%|██        | 6/30 [00:29<01:56,  4.83s/it]\u001b[A\n",
      "Training:  23%|██▎       | 7/30 [00:34<01:50,  4.81s/it]\u001b[A\n",
      "Training:  27%|██▋       | 8/30 [00:38<01:43,  4.69s/it]\u001b[A\n",
      "Training:  30%|███       | 9/30 [00:43<01:42,  4.88s/it]\u001b[A\n",
      "Training:  33%|███▎      | 10/30 [00:48<01:34,  4.74s/it]\u001b[A\n",
      "Training:  37%|███▋      | 11/30 [00:52<01:28,  4.64s/it]\u001b[A\n",
      "Training:  40%|████      | 12/30 [00:57<01:27,  4.85s/it]\u001b[A\n",
      "Training:  43%|████▎     | 13/30 [01:02<01:20,  4.72s/it]\u001b[A\n",
      "Training:  47%|████▋     | 14/30 [01:06<01:13,  4.62s/it]\u001b[A\n",
      "Training:  50%|█████     | 15/30 [01:12<01:12,  4.83s/it]\u001b[A\n",
      "Training:  53%|█████▎    | 16/30 [01:16<01:06,  4.72s/it]\u001b[A\n",
      "Training:  57%|█████▋    | 17/30 [01:21<01:00,  4.69s/it]\u001b[A\n",
      "Training:  60%|██████    | 18/30 [01:26<00:57,  4.80s/it]\u001b[A\n",
      "Training:  63%|██████▎   | 19/30 [01:30<00:51,  4.68s/it]\u001b[A\n",
      "Training:  67%|██████▋   | 20/30 [01:35<00:47,  4.74s/it]\u001b[A\n",
      "Training:  70%|███████   | 21/30 [01:40<00:42,  4.77s/it]\u001b[A\n",
      "Training:  73%|███████▎  | 22/30 [01:44<00:37,  4.66s/it]\u001b[A\n",
      "Training:  77%|███████▋  | 23/30 [01:49<00:33,  4.81s/it]\u001b[A\n",
      "Training:  80%|████████  | 24/30 [01:54<00:28,  4.72s/it]\u001b[A\n",
      "Training:  83%|████████▎ | 25/30 [01:58<00:23,  4.62s/it]\u001b[A\n",
      "Training:  87%|████████▋ | 26/30 [02:04<00:19,  4.82s/it]\u001b[A\n",
      "Training:  90%|█████████ | 27/30 [02:08<00:14,  4.70s/it]\u001b[A\n",
      "Training:  93%|█████████▎| 28/30 [02:12<00:09,  4.61s/it]\u001b[A\n",
      "Training:  97%|█████████▋| 29/30 [02:18<00:04,  4.83s/it]\u001b[A\n",
      "Training: 100%|██████████| 30/30 [02:22<00:00,  4.76s/it]\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished VAE in 145.49 seconds\n",
      "Running unsupervised model: HBOS on tweets_hate_speech_detection_distiluse-base-multilingual-cased-v1\n",
      "→ Running HBOS [unsupervised]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "\rBenchmarking dataset/encoder pairs:  33%|███▎      | 1/3 [12:54<25:48, 774.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished HBOS in 6.68 seconds\n",
      "\n",
      "Processing: tweets_hate_speech_detection_distiluse-base-multilingual-cased-v2\n",
      "✅ Selected normal label (inlier → 0): 0\n",
      "⚠️  Anomaly labels (outlier → 1): [-1, 1]\n",
      "Shape of embeddings: (31206, 512)\n",
      "Shape of labels: (31206,)\n",
      "Number of anomalies: 1486\n",
      "Number of normal: 29720\n",
      "Percentage of anomalies: 0.047619047619047616\n",
      "Shape of texts: 31206\n",
      "Train set: (24964, 512) (with 1189 outliers and 23775 inliers)\n",
      "Test  set: (6242, 512) (with 297 outliers and 5945 inliers)\n",
      "Running semi model: DevNet on tweets_hate_speech_detection_distiluse-base-multilingual-cased-v2\n",
      "→ Running DevNet (uncertainty_model=False)\n",
      "Start Training...\n",
      "ensemble size: 1\n",
      "MLPnet(\n",
      "  (network): Sequential(\n",
      "    (0): LinearBlock(\n",
      "      (linear): Linear(in_features=512, out_features=100, bias=False)\n",
      "      (act_layer): ReLU()\n",
      "    )\n",
      "    (1): LinearBlock(\n",
      "      (linear): Linear(in_features=100, out_features=50, bias=False)\n",
      "      (act_layer): ReLU()\n",
      "    )\n",
      "    (2): LinearBlock(\n",
      "      (linear): Linear(in_features=50, out_features=1, bias=False)\n",
      "      (act_layer): Identity()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "epoch  1, training loss: 2.483116, time: 1.3s\n",
      "epoch 10, training loss: 2.474618, time: 1.0s\n",
      "epoch 20, training loss: 2.488548, time: 1.0s\n",
      "epoch 30, training loss: 2.514030, time: 1.0s\n",
      "epoch 40, training loss: 2.513389, time: 1.2s\n",
      "epoch 50, training loss: 2.522998, time: 1.0s\n",
      "epoch 60, training loss: 2.472076, time: 1.0s\n",
      "epoch 70, training loss: 2.464407, time: 1.0s\n",
      "epoch 80, training loss: 2.482161, time: 1.0s\n",
      "epoch 90, training loss: 2.499419, time: 1.2s\n",
      "epoch100, training loss: 2.510195, time: 1.0s\n",
      "Start Inference on the training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "testing:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "testing:  40%|████      | 157/391 [00:00<00:00, 1569.86it/s]\u001b[A\n",
      "testing: 100%|██████████| 391/391 [00:00<00:00, 1527.23it/s]\n",
      "\n",
      "testing:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "testing:  40%|███▉      | 155/391 [00:00<00:00, 1545.80it/s]\u001b[A\n",
      "testing: 100%|██████████| 391/391 [00:00<00:00, 1400.77it/s]\n",
      "\n",
      "testing: 100%|██████████| 98/98 [00:00<00:00, 1532.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished DevNet in 104.68 seconds\n",
      "Running semi model: DeepSAD on tweets_hate_speech_detection_distiluse-base-multilingual-cased-v2\n",
      "→ Running DeepSAD (uncertainty_model=False)\n",
      "Start Training...\n",
      "ensemble size: 1\n",
      "training data counter: Counter({0: 23775, -1: 1189})\n",
      "MLPnet(\n",
      "  (network): Sequential(\n",
      "    (0): LinearBlock(\n",
      "      (linear): Linear(in_features=512, out_features=100, bias=False)\n",
      "      (act_layer): ReLU()\n",
      "    )\n",
      "    (1): LinearBlock(\n",
      "      (linear): Linear(in_features=100, out_features=50, bias=False)\n",
      "      (act_layer): ReLU()\n",
      "    )\n",
      "    (2): LinearBlock(\n",
      "      (linear): Linear(in_features=50, out_features=128, bias=False)\n",
      "      (act_layer): Identity()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "epoch  1, training loss: 0.871954, time: 1.0s\n",
      "epoch 10, training loss: 0.202095, time: 0.9s\n",
      "epoch 20, training loss: 0.147886, time: 0.9s\n",
      "epoch 30, training loss: 0.110564, time: 1.2s\n",
      "epoch 40, training loss: 0.094747, time: 0.9s\n",
      "epoch 50, training loss: 0.136676, time: 1.0s\n",
      "epoch 60, training loss: 0.092802, time: 0.9s\n",
      "epoch 70, training loss: 0.104434, time: 1.2s\n",
      "epoch 80, training loss: 0.226140, time: 0.9s\n",
      "epoch 90, training loss: 0.091549, time: 0.9s\n",
      "epoch100, training loss: 0.112942, time: 1.0s\n",
      "Start Inference on the training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "testing:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "testing:  37%|███▋      | 145/391 [00:00<00:00, 1441.79it/s]\u001b[A\n",
      "testing: 100%|██████████| 391/391 [00:00<00:00, 1424.50it/s]\n",
      "\n",
      "testing:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "testing:  36%|███▌      | 140/391 [00:00<00:00, 1391.39it/s]\u001b[A\n",
      "testing: 100%|██████████| 391/391 [00:00<00:00, 1325.46it/s]\n",
      "\n",
      "testing: 100%|██████████| 98/98 [00:00<00:00, 1381.95it/s]\n",
      "/usr/local/lib/python3.11/dist-packages/pyod/models/base.py:554: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished DeepSAD in 100.95 seconds\n",
      "Running semi model: XGBOD on tweets_hate_speech_detection_distiluse-base-multilingual-cased-v2\n",
      "→ Running XGBOD (uncertainty_model=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [12:48:47] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished XGBOD in 52.86 seconds\n",
      "Running semi model: MLP on tweets_hate_speech_detection_distiluse-base-multilingual-cased-v2\n",
      "→ Running MLP (uncertainty_model=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished MLP in 72.21 seconds\n",
      "Running unsupervised model: IForest on tweets_hate_speech_detection_distiluse-base-multilingual-cased-v2\n",
      "→ Running IForest [unsupervised]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished IForest in 1.10 seconds\n",
      "Running unsupervised model: LOF on tweets_hate_speech_detection_distiluse-base-multilingual-cased-v2\n",
      "→ Running LOF [unsupervised]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished LOF in 41.15 seconds\n",
      "Running unsupervised model: DeepSVDD on tweets_hate_speech_detection_distiluse-base-multilingual-cased-v2\n",
      "→ Running DeepSVDD [unsupervised]\n",
      "Start Training...\n",
      "ensemble size: 1\n",
      "MLPnet(\n",
      "  (network): Sequential(\n",
      "    (0): LinearBlock(\n",
      "      (linear): Linear(in_features=512, out_features=100, bias=False)\n",
      "      (act_layer): ReLU()\n",
      "    )\n",
      "    (1): LinearBlock(\n",
      "      (linear): Linear(in_features=100, out_features=50, bias=False)\n",
      "      (act_layer): ReLU()\n",
      "    )\n",
      "    (2): LinearBlock(\n",
      "      (linear): Linear(in_features=50, out_features=128, bias=False)\n",
      "      (act_layer): Identity()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "epoch  1, training loss: 0.046767, time: 0.8s\n",
      "epoch 10, training loss: 0.001636, time: 0.7s\n",
      "epoch 20, training loss: 0.001078, time: 0.8s\n",
      "epoch 30, training loss: 0.001007, time: 0.9s\n",
      "epoch 40, training loss: 0.000918, time: 0.7s\n",
      "epoch 50, training loss: 0.000842, time: 0.7s\n",
      "epoch 60, training loss: 0.000762, time: 0.7s\n",
      "epoch 70, training loss: 0.000790, time: 0.7s\n",
      "epoch 80, training loss: 0.000682, time: 0.9s\n",
      "epoch 90, training loss: 0.000776, time: 0.8s\n",
      "epoch100, training loss: 0.000741, time: 1.0s\n",
      "Start Inference on the training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "testing:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "testing:  38%|███▊      | 150/391 [00:00<00:00, 1498.82it/s]\u001b[A\n",
      "testing: 100%|██████████| 391/391 [00:00<00:00, 1405.84it/s]\n",
      "\n",
      "testing:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "testing:  36%|███▋      | 142/391 [00:00<00:00, 1412.44it/s]\u001b[A\n",
      "testing: 100%|██████████| 391/391 [00:00<00:00, 1370.30it/s]\n",
      "\n",
      "testing: 100%|██████████| 98/98 [00:00<00:00, 1417.71it/s]\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished DeepSVDD in 79.92 seconds\n",
      "Running unsupervised model: OCSVM on tweets_hate_speech_detection_distiluse-base-multilingual-cased-v2\n",
      "→ Running OCSVM [unsupervised]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished OCSVM in 55.62 seconds\n",
      "Running unsupervised model: OneClassSVM on tweets_hate_speech_detection_distiluse-base-multilingual-cased-v2\n",
      "→ Running OneClassSVM [unsupervised]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished OneClassSVM in 42.28 seconds\n",
      "Running unsupervised model: AutoEncoder on tweets_hate_speech_detection_distiluse-base-multilingual-cased-v2\n",
      "→ Running AutoEncoder [unsupervised]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Training:  10%|█         | 1/10 [00:02<00:26,  2.99s/it]\u001b[A\n",
      "Training:  20%|██        | 2/10 [00:05<00:23,  2.99s/it]\u001b[A\n",
      "Training:  30%|███       | 3/10 [00:08<00:20,  2.98s/it]\u001b[A\n",
      "Training:  40%|████      | 4/10 [00:12<00:19,  3.29s/it]\u001b[A\n",
      "Training:  50%|█████     | 5/10 [00:15<00:15,  3.18s/it]\u001b[A\n",
      "Training:  60%|██████    | 6/10 [00:18<00:12,  3.11s/it]\u001b[A\n",
      "Training:  70%|███████   | 7/10 [00:21<00:09,  3.07s/it]\u001b[A\n",
      "Training:  80%|████████  | 8/10 [00:25<00:06,  3.27s/it]\u001b[A\n",
      "Training:  90%|█████████ | 9/10 [00:28<00:03,  3.21s/it]\u001b[A\n",
      "Training: 100%|██████████| 10/10 [00:31<00:00,  3.14s/it]\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished AutoEncoder in 33.86 seconds\n",
      "Running unsupervised model: VAE on tweets_hate_speech_detection_distiluse-base-multilingual-cased-v2\n",
      "→ Running VAE [unsupervised]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "Training:   3%|▎         | 1/30 [00:05<02:31,  5.23s/it]\u001b[A\n",
      "Training:   7%|▋         | 2/30 [00:09<02:12,  4.74s/it]\u001b[A\n",
      "Training:  10%|█         | 3/30 [00:13<02:03,  4.57s/it]\u001b[A\n",
      "Training:  13%|█▎        | 4/30 [00:19<02:05,  4.83s/it]\u001b[A\n",
      "Training:  17%|█▋        | 5/30 [00:23<01:56,  4.66s/it]\u001b[A\n",
      "Training:  20%|██        | 6/30 [00:27<01:49,  4.57s/it]\u001b[A\n",
      "Training:  23%|██▎       | 7/30 [00:33<01:49,  4.78s/it]\u001b[A\n",
      "Training:  27%|██▋       | 8/30 [00:37<01:42,  4.64s/it]\u001b[A\n",
      "Training:  30%|███       | 9/30 [00:42<01:36,  4.61s/it]\u001b[A\n",
      "Training:  33%|███▎      | 10/30 [00:47<01:35,  4.77s/it]\u001b[A\n",
      "Training:  37%|███▋      | 11/30 [00:51<01:28,  4.66s/it]\u001b[A\n",
      "Training:  40%|████      | 12/30 [00:56<01:24,  4.72s/it]\u001b[A\n",
      "Training:  43%|████▎     | 13/30 [01:01<01:20,  4.74s/it]\u001b[A\n",
      "Training:  47%|████▋     | 14/30 [01:05<01:14,  4.63s/it]\u001b[A\n",
      "Training:  50%|█████     | 15/30 [01:10<01:11,  4.74s/it]\u001b[A\n",
      "Training:  53%|█████▎    | 16/30 [01:15<01:05,  4.69s/it]\u001b[A\n",
      "Training:  57%|█████▋    | 17/30 [01:19<00:59,  4.59s/it]\u001b[A\n",
      "Training:  60%|██████    | 18/30 [01:24<00:57,  4.80s/it]\u001b[A\n",
      "Training:  63%|██████▎   | 19/30 [01:29<00:51,  4.67s/it]\u001b[A\n",
      "Training:  67%|██████▋   | 20/30 [01:33<00:45,  4.59s/it]\u001b[A\n",
      "Training:  70%|███████   | 21/30 [01:38<00:42,  4.77s/it]\u001b[A\n",
      "Training:  73%|███████▎  | 22/30 [01:43<00:37,  4.66s/it]\u001b[A\n",
      "Training:  77%|███████▋  | 23/30 [01:47<00:32,  4.58s/it]\u001b[A\n",
      "Training:  80%|████████  | 24/30 [01:52<00:28,  4.78s/it]\u001b[A\n",
      "Training:  83%|████████▎ | 25/30 [01:57<00:23,  4.69s/it]\u001b[A\n",
      "Training:  87%|████████▋ | 26/30 [02:01<00:18,  4.68s/it]\u001b[A\n",
      "Training:  90%|█████████ | 27/30 [02:07<00:14,  4.78s/it]\u001b[A\n",
      "Training:  93%|█████████▎| 28/30 [02:11<00:09,  4.65s/it]\u001b[A\n",
      "Training:  97%|█████████▋| 29/30 [02:16<00:04,  4.72s/it]\u001b[A\n",
      "Training: 100%|██████████| 30/30 [02:20<00:00,  4.70s/it]\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished VAE in 143.78 seconds\n",
      "Running unsupervised model: HBOS on tweets_hate_speech_detection_distiluse-base-multilingual-cased-v2\n",
      "→ Running HBOS [unsupervised]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "\rBenchmarking dataset/encoder pairs:  67%|██████▋   | 2/3 [25:14<12:34, 754.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished HBOS in 3.08 seconds\n",
      "\n",
      "Processing: tweets_hate_speech_detection_xlm-roberta-large\n",
      "✅ Selected normal label (inlier → 0): 0\n",
      "⚠️  Anomaly labels (outlier → 1): [-1, 1]\n",
      "Shape of embeddings: (31206, 1024)\n",
      "Shape of labels: (31206,)\n",
      "Number of anomalies: 1486\n",
      "Number of normal: 29720\n",
      "Percentage of anomalies: 0.047619047619047616\n",
      "Shape of texts: 31206\n",
      "Train set: (24964, 1024) (with 1189 outliers and 23775 inliers)\n",
      "Test  set: (6242, 1024) (with 297 outliers and 5945 inliers)\n",
      "Running semi model: DevNet on tweets_hate_speech_detection_xlm-roberta-large\n",
      "→ Running DevNet (uncertainty_model=False)\n",
      "Start Training...\n",
      "ensemble size: 1\n",
      "MLPnet(\n",
      "  (network): Sequential(\n",
      "    (0): LinearBlock(\n",
      "      (linear): Linear(in_features=1024, out_features=100, bias=False)\n",
      "      (act_layer): ReLU()\n",
      "    )\n",
      "    (1): LinearBlock(\n",
      "      (linear): Linear(in_features=100, out_features=50, bias=False)\n",
      "      (act_layer): ReLU()\n",
      "    )\n",
      "    (2): LinearBlock(\n",
      "      (linear): Linear(in_features=50, out_features=1, bias=False)\n",
      "      (act_layer): Identity()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "epoch  1, training loss: 2.482046, time: 1.1s\n",
      "epoch 10, training loss: 2.520800, time: 1.1s\n",
      "epoch 20, training loss: 2.511057, time: 1.2s\n",
      "epoch 30, training loss: 2.502955, time: 1.0s\n",
      "epoch 40, training loss: 2.516145, time: 1.0s\n",
      "epoch 50, training loss: 2.500943, time: 1.0s\n",
      "epoch 60, training loss: 2.492467, time: 1.2s\n",
      "epoch 70, training loss: 2.472815, time: 1.1s\n",
      "epoch 80, training loss: 2.499394, time: 1.0s\n",
      "epoch 90, training loss: 2.492008, time: 1.0s\n",
      "epoch100, training loss: 2.508444, time: 1.0s\n",
      "Start Inference on the training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "testing:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "testing:  38%|███▊      | 147/391 [00:00<00:00, 1465.91it/s]\u001b[A\n",
      "testing: 100%|██████████| 391/391 [00:00<00:00, 1403.64it/s]\n",
      "\n",
      "testing:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "testing:  37%|███▋      | 145/391 [00:00<00:00, 1440.32it/s]\u001b[A\n",
      "testing: 100%|██████████| 391/391 [00:00<00:00, 1306.45it/s]\n",
      "\n",
      "testing: 100%|██████████| 98/98 [00:00<00:00, 1428.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished DevNet in 106.42 seconds\n",
      "Running semi model: DeepSAD on tweets_hate_speech_detection_xlm-roberta-large\n",
      "→ Running DeepSAD (uncertainty_model=False)\n",
      "Start Training...\n",
      "ensemble size: 1\n",
      "training data counter: Counter({0: 23775, -1: 1189})\n",
      "MLPnet(\n",
      "  (network): Sequential(\n",
      "    (0): LinearBlock(\n",
      "      (linear): Linear(in_features=1024, out_features=100, bias=False)\n",
      "      (act_layer): ReLU()\n",
      "    )\n",
      "    (1): LinearBlock(\n",
      "      (linear): Linear(in_features=100, out_features=50, bias=False)\n",
      "      (act_layer): ReLU()\n",
      "    )\n",
      "    (2): LinearBlock(\n",
      "      (linear): Linear(in_features=50, out_features=128, bias=False)\n",
      "      (act_layer): Identity()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "epoch  1, training loss: 0.982176, time: 0.9s\n",
      "epoch 10, training loss: 0.479647, time: 1.0s\n",
      "epoch 20, training loss: 0.571948, time: 1.1s\n",
      "epoch 30, training loss: 0.598953, time: 0.9s\n",
      "epoch 40, training loss: 1.154663, time: 0.9s\n",
      "epoch 50, training loss: 0.508224, time: 0.9s\n",
      "epoch 60, training loss: 0.903054, time: 1.2s\n",
      "epoch 70, training loss: 0.717481, time: 0.9s\n",
      "epoch 80, training loss: 0.583457, time: 0.9s\n",
      "epoch 90, training loss: 0.706952, time: 1.0s\n",
      "epoch100, training loss: 0.657501, time: 1.2s\n",
      "Start Inference on the training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "testing:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "testing:  26%|██▌       | 100/391 [00:00<00:00, 995.98it/s]\u001b[A\n",
      "testing:  52%|█████▏    | 204/391 [00:00<00:00, 1019.66it/s]\u001b[A\n",
      "testing: 100%|██████████| 391/391 [00:00<00:00, 964.02it/s]\n",
      "\n",
      "testing:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "testing:  22%|██▏       | 86/391 [00:00<00:00, 859.22it/s]\u001b[A\n",
      "testing:  44%|████▍     | 172/391 [00:00<00:00, 845.68it/s]\u001b[A\n",
      "testing:  69%|██████▉   | 270/391 [00:00<00:00, 903.06it/s]\u001b[A\n",
      "testing: 100%|██████████| 391/391 [00:00<00:00, 873.81it/s]\n",
      "\n",
      "testing:   0%|          | 0/98 [00:00<?, ?it/s]\u001b[A\n",
      "testing: 100%|██████████| 98/98 [00:00<00:00, 826.73it/s]\n",
      "/usr/local/lib/python3.11/dist-packages/pyod/models/base.py:554: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished DeepSAD in 102.45 seconds\n",
      "Running semi model: XGBOD on tweets_hate_speech_detection_xlm-roberta-large\n",
      "→ Running XGBOD (uncertainty_model=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:01:25] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished XGBOD in 97.75 seconds\n",
      "Running semi model: MLP on tweets_hate_speech_detection_xlm-roberta-large\n",
      "→ Running MLP (uncertainty_model=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished MLP in 315.85 seconds\n",
      "Running unsupervised model: IForest on tweets_hate_speech_detection_xlm-roberta-large\n",
      "→ Running IForest [unsupervised]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished IForest in 1.38 seconds\n",
      "Running unsupervised model: LOF on tweets_hate_speech_detection_xlm-roberta-large\n",
      "→ Running LOF [unsupervised]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished LOF in 76.86 seconds\n",
      "Running unsupervised model: DeepSVDD on tweets_hate_speech_detection_xlm-roberta-large\n",
      "→ Running DeepSVDD [unsupervised]\n",
      "Start Training...\n",
      "ensemble size: 1\n",
      "MLPnet(\n",
      "  (network): Sequential(\n",
      "    (0): LinearBlock(\n",
      "      (linear): Linear(in_features=1024, out_features=100, bias=False)\n",
      "      (act_layer): ReLU()\n",
      "    )\n",
      "    (1): LinearBlock(\n",
      "      (linear): Linear(in_features=100, out_features=50, bias=False)\n",
      "      (act_layer): ReLU()\n",
      "    )\n",
      "    (2): LinearBlock(\n",
      "      (linear): Linear(in_features=50, out_features=128, bias=False)\n",
      "      (act_layer): Identity()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "epoch  1, training loss: 0.003749, time: 0.8s\n",
      "epoch 10, training loss: 0.000119, time: 0.8s\n",
      "epoch 20, training loss: 0.000095, time: 1.0s\n",
      "epoch 30, training loss: 0.000043, time: 0.8s\n",
      "epoch 40, training loss: 0.000037, time: 0.8s\n",
      "epoch 50, training loss: 0.000043, time: 0.8s\n",
      "epoch 60, training loss: 0.000049, time: 0.8s\n",
      "epoch 70, training loss: 0.000032, time: 0.9s\n",
      "epoch 80, training loss: 0.000031, time: 0.8s\n",
      "epoch 90, training loss: 0.000032, time: 0.8s\n",
      "epoch100, training loss: 0.000024, time: 0.9s\n",
      "Start Inference on the training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "testing:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "testing:  23%|██▎       | 88/391 [00:00<00:00, 873.00it/s]\u001b[A\n",
      "testing:  46%|████▌     | 180/391 [00:00<00:00, 899.58it/s]\u001b[A\n",
      "testing:  70%|██████▉   | 272/391 [00:00<00:00, 905.54it/s]\u001b[A\n",
      "testing: 100%|██████████| 391/391 [00:00<00:00, 889.92it/s]\n",
      "\n",
      "testing:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "testing:  26%|██▌       | 101/391 [00:00<00:00, 1007.90it/s]\u001b[A\n",
      "testing:  52%|█████▏    | 202/391 [00:00<00:00, 926.45it/s] \u001b[A\n",
      "testing:  76%|███████▌  | 296/391 [00:00<00:00, 910.94it/s]\u001b[A\n",
      "testing: 100%|██████████| 391/391 [00:00<00:00, 890.14it/s]\n",
      "\n",
      "testing:   0%|          | 0/98 [00:00<?, ?it/s]\u001b[A\n",
      "testing: 100%|██████████| 98/98 [00:00<00:00, 799.45it/s]\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished DeepSVDD in 83.98 seconds\n",
      "Running unsupervised model: OCSVM on tweets_hate_speech_detection_xlm-roberta-large\n",
      "→ Running OCSVM [unsupervised]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished OCSVM in 106.46 seconds\n",
      "Running unsupervised model: OneClassSVM on tweets_hate_speech_detection_xlm-roberta-large\n",
      "→ Running OneClassSVM [unsupervised]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished OneClassSVM in 81.22 seconds\n",
      "Running unsupervised model: AutoEncoder on tweets_hate_speech_detection_xlm-roberta-large\n",
      "→ Running AutoEncoder [unsupervised]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Training:  10%|█         | 1/10 [00:03<00:31,  3.46s/it]\u001b[A\n",
      "Training:  20%|██        | 2/10 [00:06<00:27,  3.47s/it]\u001b[A\n",
      "Training:  30%|███       | 3/10 [00:10<00:25,  3.68s/it]\u001b[A\n",
      "Training:  40%|████      | 4/10 [00:14<00:22,  3.74s/it]\u001b[A\n",
      "Training:  50%|█████     | 5/10 [00:18<00:18,  3.62s/it]\u001b[A\n",
      "Training:  60%|██████    | 6/10 [00:21<00:14,  3.56s/it]\u001b[A\n",
      "Training:  70%|███████   | 7/10 [00:25<00:11,  3.80s/it]\u001b[A\n",
      "Training:  80%|████████  | 8/10 [00:29<00:07,  3.66s/it]\u001b[A\n",
      "Training:  90%|█████████ | 9/10 [00:32<00:03,  3.57s/it]\u001b[A\n",
      "Training: 100%|██████████| 10/10 [00:36<00:00,  3.61s/it]\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished AutoEncoder in 40.00 seconds\n",
      "Running unsupervised model: VAE on tweets_hate_speech_detection_xlm-roberta-large\n",
      "→ Running VAE [unsupervised]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "Training:   3%|▎         | 1/30 [00:04<02:17,  4.73s/it]\u001b[A\n",
      "Training:   7%|▋         | 2/30 [00:09<02:12,  4.73s/it]\u001b[A\n",
      "Training:  10%|█         | 3/30 [00:15<02:19,  5.17s/it]\u001b[A\n",
      "Training:  13%|█▎        | 4/30 [00:19<02:09,  4.98s/it]\u001b[A\n",
      "Training:  17%|█▋        | 5/30 [00:24<02:05,  5.01s/it]\u001b[A\n",
      "Training:  20%|██        | 6/30 [00:30<02:01,  5.06s/it]\u001b[A\n",
      "Training:  23%|██▎       | 7/30 [00:34<01:53,  4.93s/it]\u001b[A\n",
      "Training:  27%|██▋       | 8/30 [00:40<01:52,  5.12s/it]\u001b[A\n",
      "Training:  30%|███       | 9/30 [00:44<01:43,  4.95s/it]\u001b[A\n",
      "Training:  33%|███▎      | 10/30 [00:49<01:37,  4.85s/it]\u001b[A\n",
      "Training:  37%|███▋      | 11/30 [00:54<01:36,  5.06s/it]\u001b[A\n",
      "Training:  40%|████      | 12/30 [00:59<01:28,  4.94s/it]\u001b[A\n",
      "Training:  43%|████▎     | 13/30 [01:04<01:23,  4.91s/it]\u001b[A\n",
      "Training:  47%|████▋     | 14/30 [01:09<01:20,  5.02s/it]\u001b[A\n",
      "Training:  50%|█████     | 15/30 [01:14<01:13,  4.92s/it]\u001b[A\n",
      "Training:  53%|█████▎    | 16/30 [01:19<01:10,  5.02s/it]\u001b[A\n",
      "Training:  57%|█████▋    | 17/30 [01:24<01:05,  5.01s/it]\u001b[A\n",
      "Training:  60%|██████    | 18/30 [01:29<00:58,  4.89s/it]\u001b[A\n",
      "Training:  63%|██████▎   | 19/30 [01:34<00:55,  5.07s/it]\u001b[A\n",
      "Training:  67%|██████▋   | 20/30 [01:39<00:49,  4.95s/it]\u001b[A\n",
      "Training:  70%|███████   | 21/30 [01:44<00:43,  4.86s/it]\u001b[A\n",
      "Training:  73%|███████▎  | 22/30 [01:49<00:40,  5.07s/it]\u001b[A\n",
      "Training:  77%|███████▋  | 23/30 [01:54<00:34,  4.94s/it]\u001b[A\n",
      "Training:  80%|████████  | 24/30 [01:59<00:30,  5.00s/it]\u001b[A\n",
      "Training:  83%|████████▎ | 25/30 [02:04<00:25,  5.02s/it]\u001b[A\n",
      "Training:  87%|████████▋ | 26/30 [02:09<00:19,  4.93s/it]\u001b[A\n",
      "Training:  90%|█████████ | 27/30 [02:14<00:15,  5.11s/it]\u001b[A\n",
      "Training:  93%|█████████▎| 28/30 [02:19<00:09,  4.97s/it]\u001b[A\n",
      "Training:  97%|█████████▋| 29/30 [02:24<00:04,  4.89s/it]\u001b[A\n",
      "Training: 100%|██████████| 30/30 [02:29<00:00,  4.99s/it]\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished VAE in 152.82 seconds\n",
      "Running unsupervised model: HBOS on tweets_hate_speech_detection_xlm-roberta-large\n",
      "→ Running HBOS [unsupervised]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "Benchmarking dataset/encoder pairs: 100%|██████████| 3/3 [44:52<00:00, 897.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished HBOS in 6.36 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_combinations = sum(\n",
    "    len(cfg[\"datasets\"]) * len(cfg[\"encoders\"])\n",
    "    for cfg in config.values()\n",
    ")\n",
    "\n",
    "with tqdm(total=total_combinations, desc=\"Benchmarking dataset/encoder pairs\") as pbar:\n",
    "    for lang, lang_config in config.items():\n",
    "        for dataset_name in lang_config[\"datasets\"]:\n",
    "            dataset_short = dataset_name.split(\"/\")[-1]\n",
    "\n",
    "            for encoder_name in lang_config[\"encoders\"]:\n",
    "                encoder_short = encoder_name.split(\"/\")[-1]\n",
    "                combo_name = f\"{dataset_short}_{encoder_short}\"\n",
    "\n",
    "                try:\n",
    "                    print(f\"\\nProcessing: {combo_name}\")\n",
    "                    texts_df = pd.read_parquet(os.path.join(project_path, f\"data/texts_{dataset_short}.parquet\"))\n",
    "                    labels_df = pd.read_parquet(os.path.join(project_path, f\"data/labels_{dataset_short}.parquet\"))\n",
    "                    embeddings_df = pd.read_parquet(os.path.join(project_path, f\"data/embeddings_{dataset_short}_{encoder_short}.parquet\"))\n",
    "\n",
    "                    labeled_anomalies_df = label_normal_vs_anomaly(labels_df, as_df=True)\n",
    "\n",
    "                    texts, labels, embeddings = adjust_contamination(\n",
    "                        texts=texts_df.squeeze().tolist(),\n",
    "                        labels=labeled_anomalies_df.squeeze().values,\n",
    "                        embeddings=embeddings_df.values,\n",
    "                        perc_anomalous=0.05\n",
    "                    )\n",
    "\n",
    "                    pretty_print_data_info(texts, labels, embeddings)\n",
    "                    x_train, x_test, y_train, y_test = split_data(embeddings, labels, random_state=42)\n",
    "\n",
    "                    # Loop through model groups (semi + unsupervised)\n",
    "                    for group_name, group_info in model_groups.items():\n",
    "                        models = group_info[\"models\"]\n",
    "                        benchmark_fn = group_info[\"benchmark_fn\"]\n",
    "                        extra_args = group_info[\"extra_args\"]\n",
    "                        wrap_model = group_info[\"wrap_model\"]\n",
    "\n",
    "                        for model_name, model_fn in models.items():\n",
    "                            print(f\"Running {group_name} model: {model_name} on {combo_name}\")\n",
    "                            start_time = time.time()\n",
    "\n",
    "                            benchmark_fn(\n",
    "                                x_train, y_train, x_test, y_test,\n",
    "                                model_constructor=wrap_model(model_name, model_fn),\n",
    "                                dataset_name=combo_name,\n",
    "                                **extra_args\n",
    "                            )\n",
    "\n",
    "                            print(f\"✅ Finished {model_name} in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"[❌ ERROR] Failed processing {combo_name}: {e}\")\n",
    "\n",
    "                pbar.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Z5GZ2BuZYR3q",
   "metadata": {
    "id": "Z5GZ2BuZYR3q"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
